---
title: "Doc-React: Multi-page Heterogeneous Document Question-Answering."
collection: publications
permalink: /publication/2025-acl
excerpt: 'Accepted at ACL 2025.'
date: 2025-07-01
venue: 'Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)'
paperurl: 'https://aclanthology.org/2025.acl-short.6/'
citation: 'Junda Wu, Yu Xia, Tong Yu, Xiang Chen, Sai Sree Harsha, Akash V Maharaj, Ruiyi Zhang, <strong>Victor S. Bursztyn</strong>, Sungchul Kim, Ryan A. Rossi, Julian McAuley, Yunyao Li, and Ritwik Sinha. 2025. Doc-React: Multi-page Heterogeneous Document Question-answering. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 67â€“78, Vienna, Austria. Association for Computational Linguistics.'
---

Answering questions over multi-page, multimodal documents, including text and figures, is a critical challenge for applications that require answers to integrate information across multiple modalities and contextual dependencies. Existing methods, such as single-turn retrieval-augmented generation (RAG), struggle to retrieve fine-grained and contextually relevant information from large, heterogeneous documents, leading to suboptimal performance. Inspired by iterative frameworks like ReAct, which refine retrieval through feedback, we propose Doc-React, an adaptive iterative framework that balances information gain and uncertainty reduction at each step. Doc-React leverages InfoNCE-guided retrieval to approximate mutual information, enabling dynamic sub-query generation and refinement. A large language model (LLM) serves as both a judge and generator, providing structured feedback to iteratively improve retrieval. By combining mutual information optimization with entropy-aware selection, Doc-React systematically captures relevant multimodal content, achieving strong performance on complex QA tasks.